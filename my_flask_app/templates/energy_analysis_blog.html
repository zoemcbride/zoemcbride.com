{% extends "base.html" %}

{% block content %}

<body>

  <!-- zoemcbride.com website code -->

  <body bgcolor = "#FFFFFF">
    <h1>Time-Series Forecasting of the Spanish Energy Grid</h1>

    <h4>October 2023</h4>
    <div class="reading-time"><i>Approximate reading time: {{ reading_time }} minute{{ 's' if reading_time != 1 else '' }}
    </div></i></p>
          

    [WIP] Follow along on <a href="https://github.com/zoemcbride/energy-data-lab">my Github repo</a>!
    <p><p><p>

    <h3>Intro & Motivation</h3>
    <p class="opening-paragraph">
    My goal with this exercise was to work on my time-series forecasting skills. Particularly, I wanted to see if I could develop both a k-Nearest Neighbors (KNN) model and a Random Forest (RF) model to predict the load value on the grid at a particular point in time. </p>

    <p class="opening-paragraph">
    The data I had included the actual load value as well as the Spanish Transmission System Operator (TSO) best-in-class forecasted values. My objective was to compare both my KNN and my RF model against actual and benchmarked TSO forecasts.</p>

    <h3>Background</h3>
    <p class="opening-paragraph">
    When it comes to time-series forecasting in the energy space, models like KNN, RF, Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA (SARIMA), Prophet, Long Short-Term Memory Networks (LSTMs) and XGBoost are common. However, KNN and RF are simpler and more lightweight than these other models, and thus a good place to start. </p>

    <p class="opening-paragraph">KNN is a highly adaptable, lightweight model which adapts swiftly to new data patterns. RF on the other hand can be great for handling multiple features and also can give us a sense of the feature importance. When using a RF model, we can pinpoint the critical drivers of the energy load by using the feature importance output, and this can be a helpful tool in itself. </p>

    <h3>Data Used</h3> 
    <p class="opening-paragraph">
      Data generally from <a href="https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather/data">this Kaggle project</a>.</p>

      <p class="opening-paragraph">
      This dataset contains 4 years of electrical consumption, generation, pricing, and weather data for Spain. Consumption and generation data was retrieved from ENTSOE - a public portal for Transmission Service Operator (TSO) data. Settlement prices were obtained from the Spanish TSO Red Electric España. Weather data was purchased from the Open Weather API for the 5 largest cities in Spain and made public.</p>

      <div class='container' style='max-width:1200px; margin-left: 0; border: 2px solid rgb(136, 225, 108); border-radius: 15px;'>
  <h5>Spanish Energy Data</h5>
  <table style='width:100%; border-collapse: collapse; '>
    <tr style='border-bottom: 2px solid rgb(136, 225, 108);'>
    </tr>
    <tr>
      <td>Data was in hourly timestamps and spanned from 2015-2019</td>
    </tr>
    <tr>
      <td>Energy sources (MW) were provided and broken down by category, which I have grouped for simplicity:</td>
    </tr>
    <tr>
      <td>
        <ul>
          <li>Fossil Fuels (brown coal/lignite, coal-derived gas, gas, hard coal, oil, oil shale, fossil peat)</li>
          <li>Hydro (pumped storage consumption, run-of-river poundage, water reservoir)</li>
          <li>Wind & Solar (wind offshore, wind onshore, solar, other renewables)</li>
          <li>Nuclear</li>
          <li>Other (biomass, geothermal, marine, waste, other)</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>Other data streams included:</td>
    </tr>
    <tr>
      <td>
        <ul> 
          <li>Load Forecasts from the TSO</li>
          <li>Actual Load</li>
          <li>Forecasted Day Ahead Price from the TSO</li>
          <li>Actual Price</li>
      </ul>
    </td>
    </tr>
  </table>
  <h5>Spanish Weather Data</h5>
  <table style='width:100%; border-collapse: collapse; '>
    <tr style='border-bottom: 2px solid rgb(136, 225, 108);'>
    </tr>
    <tr>
      <td>Data for 5 Spanish cities (Valencia, Madrid, Bilbao, Barcelona, Seville) was provided in hourly timestamps and spanned from 2015-2019</td>
    </tr>
    <tr>
      <td>Data streams included: 
        <ul>
        <li>Temperature (including minimum and maximum temperature)</li>
        <li>Pressure</li>
        <li>Humidity</li>
        <li>Wind Speed (including wind direction)</li>
        <li>Rainfall (including last hour, last three hours)</li>
        <li>Snowfall (last three hours)</li>
        <li>Cloud Coverage</li>
        <li>Weather ID, Short Description, Long Description, Icon</li>
      </ul>
      </td>
    </tr>
  </table>
</div>



</p>

    <h3>Exploratory Data Analysis Insights</h3>
    <p class="opening-paragraph">
     Of all the steps to create a forecasting model, exploring the data took a large chunk of time. Particularly this was the case because some things that were uncovered in the exploratory data analysis (EDA) phase needed to be addressed in the data cleaning/pre-processing phase, which led to a bit of an iterative process. Rather than go over every single thing I analyzed (which you can see in the <a href="https://github.com/zoemcbride/energy-data-lab">energy-data-lab repo</a>), I will highlight some of the conclusions.
     
     <p class="opening-paragraph">
      <i>Data Inspection.</i> Several columns in the energy data were zero or missing throughout, meaning these columns could be dropped.  Outside of those columns, the scale of the values varied a lot, meaning normalizing or scaling the data would be important in the feature matrix. Likewise, the scale of the values in the weather data fluctuated a lot, indicating normalizing/scaling the data here will be necessary too. Thankfully, there are very few missing observations in the energy data and no missing data in the weather set. However, there are some thorny duplicate timestamps in the weather data set that will need addressing. </p>

      <img src="{{ url_for('static', filename='images/energy_patterns_over_time_grouped.png') }}" alt="Grouped Energy Generation by Source Over Time" class="center" style="
        max-width: 100%;
        max-height: 400px;
        margin-left: auto;
        margin-right: auto;
        align-self: center;
        display: block;
        ">
      <p class="opening-paragraph">
        <i>Patterns in Energy Generation Over Time.</i> Looking at the above makeup of the Spanish energy grid over time, it appears that wind and solar make up a relatively small portion overall, though upon closer inspection I found that there were occasional summer days where wind and solar made up ~50% of the grid makeup. Generally, fossil fuels make up approximately 25% of the grid, nuclear almost half ot the energy grid, and hydro, biomass, and waste are smaller but important components to the grid makeup. There are definite seasonal trends, but no clear general upwards or downwards trend in generation over time.</p>

        <img src="{{ url_for('static', filename='images/wind_solar_daily_profile.png') }}" alt="Grouped Energy Generation by Source Over Time" class="center" style="
        max-width: 100%;
        max-height: 400px;
        margin-left: auto;
        margin-right: auto;
        align-self: center;
        display: block;
        ">
        <p class="opening-paragraph">In the above graph, the shape of the wind and solar generated on a typical day is clear.  As expected, wind and solar generation peaks around the middle of the day, when the sun (and thus, solar output and windspeed) is at it's peak. Whereas a purely solar profile would approach zero through the dark hours of the evening, the presence of wind power means that there is still some renewable generation throughout that period. Based on my understanding of the energy grid and further concluded by other investigations, given this classic renewable generation shape, fossil fuel generation will appear "duck-curve" shaped. Nuclear power generation tends to be constant, due to the long ramp-up and ramp-down speeds required for nuclear power. Interestingly, the above shows that wind and solar generation did not appear to dramatically increase year-over-year, as I had expected, since renewables became cheaper to make and install. Of the two, I further investigated to find that onshore wind generation is the larger portion. </p>

        <p class="opening-paragraph"><i>Temporal Analysis.</i> I found some seasonality to the energy data. There is a bit of a duck curve shape across the year -- for example, the total load peaks in June, July and August, and again in January/February. This makes sense when you consider the weather is probably most extreme in these months, requiring additional load for heating or cooling. The global annual peak sometimes occurs in sometimes June, sometimes July, and sometimes February. Weekday load is generally higher than weekend load, with median weekday load around 30,000 MW and median weekend load around 27,000 MW. There are some outliers in the weekend load that were higher than typically expected.</p>

        <img src="{{ url_for('static', filename='images/weather_temperature_distribution_cities.png') }}" alt="Grouped Energy Generation by Source Over Time" class="center" style="
        max-width: 100%;
        max-height: 400px;
        margin-left: auto;
        margin-right: auto;
        align-self: center;
        display: block;
        ">
        <p class="opening-paragraph"><i>Temperature Distribution Across Cities.</i> Seville tends to have warmer weather than the other cities and Bilbao tends to have cooler weather than the other cities, though Madrid is very similar. Madrid's weather appears to have more of a spread than Bilbao. Overall, the weather is typically pleasant across all the cities, with most hours hovering between 50-78°F depending on the city. All the cities have had some extreme weather events -- times where the temperature exceeded 100°F, and occasionally times where the temperature approached freezing. Madrid saw days where temperature dropped well below freezing. Madrid had the widest range of values.

        <img src="{{ url_for('static', filename='images/madrid_totalload_vs_temperature.png') }}" alt="Grouped Energy Generation by Source Over Time" class="center" style="
        max-width: 100%;
        max-height: 400px;
        margin-left: auto;
        margin-right: auto;
        align-self: center;
        display: block;
        ">
        <p class="opening-paragraph"><i>Correlation Analysis: Load vs. Weather.</i> The above chart is an example of the total grid load versus the temperature for the city of Madrid. All five cities studied showed a similar pattern: there is a clear upward trend. As temperature increases, total load increases. However, there appears to also be something interesting at the lower end too: as temperature gets low, total load is occasionally higher. I explored this further in the below box plot. Clearly, the higher temperatures (here defined as >90°F) had higher loads. For a clear linear trend, these charts would appear linear too. However, instead we see that at lower temperatures (here defined as <50°F), some load data points were higher than typical temperature load data points. The median load of the lower temperature data was lower than the median load of the typical temperature data, but, the spread of the lower temperature data was wider, and a significant number of extremely cold hours actually had a greater load than the typical temperature hours. This was an interesting finding, and something I wanted to include into my feature matrix. </p>
        <img src="{{ url_for('static', filename='images/madrid_temperature_distribution.png') }}" alt="Grouped Energy Generation by Source Over Time" class="center" style="
        max-width: 100%;
        max-height: 400px;
        margin-left: auto;
        margin-right: auto;
        align-self: center;
        display: block;
        ">


    <h3>Feature Matrix Selection</h3>
    <p class="opening-paragraph">Of course, there were several iterations of feature matrices that I played with. However, given the results of the EDA as well as my domain knowledge, I had an intuition that the following features would be good starting points and I stuck to them more or less. All values in the feature matrix are scaled, either between 0 and 1, or using a standard scalar, meaning they have a mean of 0 and a standard deviation of +/- 1. Standardly scaled features could be larger or smaller than +/- 1 for values that were larger than 1 standard deviation outside of the mean and typically went to around +/-3. For KNN, scaling is critical as it is a distance-based model. For RF, scaling is less relevant but can still be used. Thus the features included:
      <ul>
        <li>Temperature (°F) for each city, scaled with a standard scalar</li>
        <li>Temperature extreme cold binary, where 1 represents extremely cold temperatures less than a cutoff (for example, 32°F, though I played with this value)</li>
        <li>Temperature extreme hot binary, where 1 represents extremely hot temperatures more than a cutoff (for example, 90°F, though I played with this value)</li>
        <li>Day type binary, with 0 for weekend, 1 for weekday</li>
        <li>Season, derived from the month, one-hot encoded as true/false for each season</li>
        <li>Hour of the day, transformed using both sin(2pi*x) and cos(2pi*x) to indicate the cyclical nature of the hour feature</li>
        <li>Previous day load, scaled with a standard scalar</li>
      </ul>
      The target vector, of course, was the total actual load. The benchmarking vector was the total load forecasted by the TSO.
    </p>

    <h3>Results</h3>
    <p class="opening-paragraph">For both KNN and RF models, I had to impose a train-test split that makes sense for time series forecasting. A typical train-test split could impose data leakage, where future time stamps are being used to predict the future. Thus, it was imperative that the training and testing split happen at least chronologically (note that an expanding or rolling window could have been used here too). I used the first 80% of values in the feature matrix for training, and the last 20% of values for testing. Then, I used a time series cross validation on the training data with 5 splits. The goal was to see how well the model performs on unseen data by evaluating it against different folds of the training set. Once hyperparameter tuning was completed across the various folds of the training set, I would then evaluate the model against the testing set.</p>

    

    <h4>K-Nearest Neighbors (KNN)</h4>
    <p class="opening-paragraph">

    <h3>Conclusions</h3>
    <p class="opening-paragraph">

<script data-goatcounter="https://zoemcbride.goatcounter.com/count"
async src="//gc.zgo.at/count.js"></script>

{% endblock %}